{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27310ad4",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "482f8e30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-28T17:23:14.268611Z",
     "start_time": "2021-09-28T17:22:32.563449Z"
    }
   },
   "outputs": [],
   "source": [
    "# jupyter notebook 同时输出多行\n",
    "import import_ipynb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup \n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cdf842",
   "metadata": {},
   "source": [
    "## 1. Requests 库\n",
    "![image-20220122170248254](https://gitee.com/lockegogo/markdown_photo/raw/master/202201221702363.png)\n",
    "\n",
    "**HTTP** 协议：超文本传输协议，是一个基于 “**请求与响应**” 模式的、无状态（两次请求间无关联）的应用层协议（高于 TTP 协议）。采用 URL 作为定位网络资源的标识。\n",
    "\n",
    "**URL**：http://host [:port] [path]\n",
    "\n",
    "- host：合法的 Internet 主机域名或 IP 地址\n",
    "- port：端口号，缺省端口为 80\n",
    "- path：请求资源的路径\n",
    "\n",
    "### 1.1 HTTP 协议对资源的操作\n",
    "\n",
    "| 方法   | 说明                                                         |\n",
    "| ------ | ------------------------------------------------------------ |\n",
    "| GET    | 请求获取 URL 位置的资源                                      |\n",
    "| HEAD   | 请求获取 URL 位置资源的响应消息报告，即**获得该资源的头部信息** |\n",
    "| POST   | 请求向 URL 位置的资源后**附加**新的数据                      |\n",
    "| PUT    | 请求向 URL 位置存储一个资源，**覆盖**原 URL 位置的资源       |\n",
    "| PATCH  | 请求局部更新 URL 位置的资源，即改变**该处资源**的部分内容    |\n",
    "| DELETE | 请求删除 URL 位置存储的资源                                  |\n",
    "\n",
    "\n",
    "资源较大时，可用 HEAD 获取头部信息；修改部分可用 PATCH，**节省网络带宽**\n",
    "\n",
    "### 1.2 Requests 库的对象\n",
    "\n",
    "以 `head()` 方法为例：获取网页头信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1088ff05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': 'Sat, 22 Jan 2022 09:11:34 GMT', 'Content-Type': 'application/json', 'Content-Length': '312', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.head('http://httpbin.org/get')\n",
    "r. headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e088ceb",
   "metadata": {},
   "source": [
    "![image-20220122171237243](https://gitee.com/lockegogo/markdown_photo/raw/master/202201221712292.png)\n",
    "\n",
    "\n",
    "Requests 库的 `post()` 方法：向 URL POST 一个字典，自动编码为 form (表单)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ac11a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value1\", \n",
      "    \"key2\": \"value2\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.26.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-61ebd200-1ccfea3d1db2e049798a2502\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"202.120.235.176\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 向 URL POST 一个字典，自动编码为 form (表单)\n",
    "payload = {'key1':'value1', 'key2':'value2'}\n",
    "r = requests.post('http://httpbin.org/post',data=payload)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ced390f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"ABC\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Content-Length\": \"3\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.26.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-61ebddc3-604fe56d406dde15029a68a5\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"202.120.235.176\", \n",
      "  \"url\": \"http://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 向 URL POST 一个字符串，自动编码为 data\n",
    "r = requests.post('http://httpbin.org/post',data='ABC')\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eafd0f2",
   "metadata": {},
   "source": [
    "Requests 库的 `put()` 方法：会将原有数据覆盖掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d0700b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"key1\": \"value2\", \n",
      "    \"key2\": \"value1\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate, br\", \n",
      "    \"Content-Length\": \"23\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.26.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-61ebde75-515c207c622287dc47313eeb\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"202.120.235.176\", \n",
      "  \"url\": \"http://httpbin.org/put\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "payload = {'key1':'value2', 'key2':'value1'}\n",
    "r = requests.put('http://httpbin.org/put',data=payload)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579573c9",
   "metadata": {},
   "source": [
    "`r = requests.get(url)`: 构造一个向服务器请求资源的 Requests 对象，服务器返回一个包含服务器资源的 **Response** 对象\n",
    "\n",
    "**Response 对象**：包含了爬虫返回的全部内容\n",
    "| 属性                | 说明                                                 |\n",
    "| ------------------- | ---------------------------------------------------- |\n",
    "| r.status_code       | HTTP 请求的返回状态，200 表示连接成功，404 表示失败  |\n",
    "| r.text              | HTTP 响应内容的字符串形式，即 url 对应的页面内容     |\n",
    "| r.recoding          | 从 HTTP header 中**猜测**的响应内容编码方式          |\n",
    "| r.apparent_encoding | 从内容中**分析**出的响应内容编码方式（备选编码方式） |\n",
    "| r.content           | HTTP 响应内容的二进制形式                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0533a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get(\"http://www.baidu.com\")\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d74688cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdea67c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'keep-alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Sat, 22 Jan 2022 12:06:56 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:36 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 返回头部信息\n",
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cde48dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\r\\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç\\x99¾åº¦ä¸\\x80ä¸\\x8bï¼\\x8cä½\\xa0å°±ç\\x9f¥é\\x81\\x93</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç\\x99¾åº¦ä¸\\x80ä¸\\x8b class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ\\x96°é\\x97»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å\\x9c°å\\x9b¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§\\x86é¢\\x91</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å\\x90§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç\\x99»å½\\x95</a> </noscript> <script>document.write(\\'<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\\'+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ \\'\" name=\"tj_login\" class=\"lb\">ç\\x99»å½\\x95</a>\\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ\\x9b´å¤\\x9aäº§å\\x93\\x81</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å\\x85³äº\\x8eç\\x99¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç\\x94¨ç\\x99¾åº¦å\\x89\\x8då¿\\x85è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æ\\x84\\x8fè§\\x81å\\x8f\\x8dé¦\\x88</a>&nbsp;äº¬ICPè¯\\x81030173å\\x8f·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\\r\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text\n",
    "# 输出中很多是乱码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c285d2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding 的编码方式是从 HTTP header 中的 charset 中获得的。若 header 中不存在 charset，则认为编码为 ISO-8859-1\n",
    "r.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47fb0932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apparent_encoding 从内容分析可能出现的编码形式（备选编码方式）\n",
    "r.apparent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5d66aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\r\\n<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>百度一下，你就知道</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=百度一下 class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>新闻</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>地图</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>视频</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>贴吧</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>登录</a> </noscript> <script>document.write(\\'<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u=\\'+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ \\'\" name=\"tj_login\" class=\"lb\">登录</a>\\');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">更多产品</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>关于百度</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>使用百度前必读</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>意见反馈</a>&nbsp;京ICP证030173号&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\\r\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.encoding = \"UTF-8\"\n",
    "r.text\n",
    "# 可以看到中文字符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d67c83",
   "metadata": {},
   "source": [
    "### 1.3 Requests 库的主要方法\n",
    "Kwargs 参数列表：`requests.request(method, url, **kwargs)`\n",
    "\n",
    "- method: 请求方式，对应 get/put/post 等 7 种\n",
    "- url：拟获取页面的 url 链接\n",
    "- **kwargs：控制访问的参数，共 13 个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c05449da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://python123.io/ws?key1=value&key2=value2\n"
     ]
    }
   ],
   "source": [
    "# params: 字典或字节序列，作为参数增加到 url 中，服务器根据参数筛选资源返回\n",
    "kv = {'key1': 'value', 'key2': 'value2'}\n",
    "r = requests.post('http://python123.io/ws', params=kv)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c47fb5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: 字典、字节序列或文件对象，作为 Request 的内容。向服务器提交资源时使用\n",
    "body = 'kswl'\n",
    "r = requests.request('POST', 'http://python123.io/ws', data=body)\n",
    "# 与 params 不同，不放入 url 链接中，放入 url 对应位置作为数据存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1bd8c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json: JSON 格式的数据，作为 Request 的内容\n",
    "kv = {'key1': 'value'}\n",
    "r = requests.request('POST', 'http://python123.io/ws', json=kv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d4f582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# header: 字典，HTTP 定制头\n",
    "hd = {'user-agent': 'Chrome/10'}\n",
    "r = requests.request('POST', 'http://python123.io/ws', headers=hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cookies: 字典或 CookieJar，Request 中的 cookie\n",
    "# auth: 元组，支持 HTTP 认证功能\n",
    "# files: 字典类型，传输文件\n",
    "fs = {'file': open('data.xls','rb')}\n",
    "r = requests.request('POST', 'http://python123.io/ws', files=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeout: 设定超时时间，单位为 s\n",
    "r = requests.request('GET', 'http://www.baidu.com', timeout=10)\n",
    "# 若超时无返回值，则会产生 timeout 异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxies: 字典类型，设定访问代理服务器，可以增加登录认证\n",
    "pxs = {'http': 'http://user:pass@10.10.10.1:1234',  #用户名及密码设置\n",
    "\t   'https': 'http://10.10.10.1:4321' }\n",
    "r = requests.request('GET', 'http://www.baidu.com', proxies=pxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow_redirects: 重定向开关，默认为 true\n",
    "# stream: 获取内容立即下载开关，默认为 true\n",
    "# verify: 认证 SSL 证书开关，默认为 true\n",
    "# cert: 本地 SSL 证书路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31117b1",
   "metadata": {},
   "source": [
    "#### 1.3.1 get 方法\n",
    "`r = requests.get(url,params=None,**kwargs)`\n",
    "\n",
    "- url：拟获取页面的 url 链接\n",
    "- params：url 中的额外参数，字典或字节流格式，可选\n",
    "- **kwargs：12 个控制访问的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ee180",
   "metadata": {},
   "source": [
    "#### 1.3.2 head 方法\n",
    "`r = requests.head(url,**kwargs)`\n",
    "\n",
    "- url：拟获取页面的 url 链接\n",
    "- **kwargs：13 个控制访问的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.head('http://httpbin.org/get')\n",
    "r.headers\n",
    "#返回头部信息\n",
    "r.text\n",
    "#返回内容为空"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4696def3",
   "metadata": {},
   "source": [
    "#### 1.3.3 post 方法\n",
    "`r = requests.post(url,**kwargs)`\n",
    "\n",
    "- url：拟更新页面的 url 链接\n",
    "- data: 字典、字节序列或文件，Requests 的内容\n",
    "- json: JSON 格式的数据，Requests 的内容\n",
    "- **kwargs：11 个控制访问的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4f2f8e",
   "metadata": {},
   "source": [
    "#### 1.3.4 put 方法\n",
    "`r = requests.put(url,**kwargs)`\n",
    "\n",
    "- url：拟更新页面的 url 链接\n",
    "- data: 字典、字节序列或文件，Requests 的内容\n",
    "- **kwargs：12 个控制访问的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cc13e",
   "metadata": {},
   "source": [
    "#### 1.3.5 patch 方法\n",
    "`r = requests.patch(url,**kwargs)`\n",
    "\n",
    "- url：拟更新页面的 url 链接\n",
    "- data: 字典、字节序列或文件，Requests 的内容\n",
    "- **kwargs：12 个控制访问的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444e9f9",
   "metadata": {},
   "source": [
    "#### 1.3.6 delete 方法\n",
    "`r = requests.delete(url,**kwargs)`\n",
    "\n",
    "- url：拟删除页面的 url 链接\n",
    "- **kwargs：13 个控制访问的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a840aa",
   "metadata": {},
   "source": [
    "### 1.4 爬取网页的通用代码框架\n",
    "**几种常见的异常**：\n",
    "| 异常                      | 说明                                          |\n",
    "| ------------------------- | --------------------------------------------- |\n",
    "| requests.ConnectionError  | 网络连接错误异常，如 DNS 查询失败、拒绝连接等 |\n",
    "| requests.HTTPError        | HTTP 错误异常                                 |\n",
    "| requests.URLRequired      | URL 缺失异常                                  |\n",
    "| requests.TooMantRedirects | 超过最大重定向次数，产生重定向异常            |\n",
    "| requests.ConnectTimeout   | **连接**远程服务器**超时**异常                |\n",
    "| requests.Timeout          | 请求 **URL 超时**，产生超时异常               |\n",
    "|r.rasis_for_status()       | 如果不是 200，产生异常 requests.HTTPError    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd7caa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def getHTMLText(url):\n",
    "    try:\n",
    "        r = requests.get(url,timeout=30)\n",
    "        r.raise_for_status() # 如果不是 200，产生requests.HTTPError异常\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "013d34af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产生异常\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    url=\"http://www.baidu.cm\"\n",
    "    # url=\"http://www.baidu.com\"\n",
    "    print(getHTMLText(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416599cb",
   "metadata": {},
   "source": [
    "### 1.5 爬虫规模\n",
    "- 爬取网页：小规模，数据量小，爬取速度不敏感。Requests 库。\n",
    "- 爬取网站：中规模，数据规模较大，爬取速度敏感。Scrapy 库。\n",
    "- 爬取全网：大规模，搜索引擎，爬取速度尤其关键。定制开发。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f195d",
   "metadata": {},
   "source": [
    "## 2. Robot 协议\n",
    "- 作用：网站告知网络爬虫哪些页面可以抓取，哪些不行。\n",
    "- 形式：在网站根目录下的 robots.txt 文件。\n",
    "- 以京东 Robots 协议为例：https://www.jd.com/robots.txt\n",
    "\n",
    "|字段|说明|\n",
    "|----|----|\n",
    "|User-agent: *|\t\t\t\t对于任意user-agent，都应遵循以下协议|\n",
    "|Disallow: /?*\t|\t\t\t不允许访问以?开头的路径|\n",
    "|Disallow: /pop/*.html\t|\t不允许访问pop路径|\n",
    "|Disallow: /pinpai/*.html?*\t|不允许访问pinpai路径|\n",
    "|User-agent: EtaoSpider\t|\t不允许该爬虫爬取|\n",
    "|User-agent: HuihuiSpider\t|不允许该爬虫爬取|\n",
    "|User-agent: GwdangSpider\t|不允许该爬虫爬取|\n",
    "|User-agent: WochachaSpider\t|不允许该爬虫爬取|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc25ee",
   "metadata": {},
   "source": [
    "**网络爬虫的限制：**\n",
    "1. 来源审查：判断 User-Agent 进行限制（技术方面）\n",
    "2. 发布公告：Robots 协议（道德方面）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746275f8",
   "metadata": {},
   "source": [
    "## 3. Request 库爬取实例\n",
    "### 3.1 京东某商品信息\n",
    "例如：https://item.jd.com/361348.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5527182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148779\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "kv = {'user-agent':'Mozilla/5.0'}\t#浏览器身份标识\n",
    "url = \"https://item.jd.com/361348.html\"\n",
    "try:\n",
    "    r = requests.get(url,headers=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccf550c",
   "metadata": {},
   "source": [
    "如果不加 `user-agent`，`r.status_code` 依然为 200，但返回结果为登录页面。因为京东对爬虫有一定的限制，如果程序中不写明 `user-agent`，访问者默认为 python 的一个程序，从而访问出错。解决办法只需将程序模拟为浏览器 *Mozilla/5.0*，即可正常访问。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa99a84",
   "metadata": {},
   "source": [
    "### 3.2 亚马逊某商品信息\n",
    "例如：https://www.amazon.cn/dp/B094XNKYY4?ref_=Oct_DLandingS_D_e0932e88_60&smid=A3TEGLC21NOO5Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9f9b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "\"onflush\");\n",
      "\n",
      "(function(d,e){function h(f,b){if(!(a.ec>a.mxe)&&f){a.ter.push(f);b=b||{};var c=f.logLevel||b.logLevel;c&&c!==k&&c!==m&&c!==n&&c!==p||a.ec++;c&&c!=k||a.ecf++;b.pageURL=\"\"+(e.location?e.location.href:\"\");b.logLevel=c;b.attribution=f.attribution||b.attribution;a.erl.push({ex:f,info:b})}}function l(a,b,c,e,g){d.ueLogError({m:a,f:b,l:c,c:\"\"+e,err:g,fromOnError:1,args:arguments},g?{attribution:g.attribution,logLevel:g.logLevel}:void 0);return!1}var k=\"FATAL\",m=\"ERROR\",n=\"WARN\",p=\"DOWNGRADED\",a={ec:0,ecf:0,\n",
      "pec:0,ts:0,erl:[],ter:[],mxe:50,startTimer:function(){a.ts++;setInterval(function(){d.ue&&a.pec<a.ec&&d.uex(\"at\");a.pec=a.ec},1E4)}};l.skipTrace=1;h.skipTrace=1;h.isStub=1;d.ueLogError=h;d.ue_err=a;e.onerror=l})(ue_csm,window);\n",
      "\n",
      "ue.stub(ue,\"event\");ue.stub(ue,\"onSushiUnload\");ue.stub(ue,\"onSushiFlush\");\n",
      "\n",
      "var ue_url='/rd/uedata',\n",
      "ue_sid='462-0510421-8773939',\n",
      "ue_mid='AAHKV2X7AFYLW',\n",
      "ue_sn='www.amazon.cn',\n",
      "ue_furl='fls-cn.amazon.cn',\n",
      "ue_surl='https://unagi.amazon.cn/1/events/co\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "kv = {'user-agent':'Mozilla/5.0'}\n",
    "cookie = {\n",
    "}\n",
    "url = \"https://www.amazon.cn/dp/B094XNKYY4?ref_=Oct_DLandingS_D_e0932e88_60&smid=A3TEGLC21NOO5Y\"\n",
    "try:\n",
    "    r = requests.get(url, cookies=cookie, headers=kv)\n",
    "    print(r.status_code)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[1000:2000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3f7a5",
   "metadata": {},
   "source": [
    "与京东略有不同，需要将 cookie 写入，否则回应信息会显示被识别为机器人。\n",
    "\n",
    "cookie 即某些网站为了辨别用户身份，进行 `Session` 跟踪而存储在用户本地终端上的数据（通常经过加密）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae5a5e",
   "metadata": {},
   "source": [
    "### 3.3 百度 360 搜索关键词提交\n",
    "- 百度的关键词接口：http://www.baidu.com/s?wd=keyword\n",
    "\n",
    "- 360的关键词接口：http://www.baidu.com/s?q=keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56a4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=python\n",
      "754212\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# 定义搜索关键词：python\n",
    "kv = {'wd': 'python'}\n",
    "url = \"http://www.baidu.com/s\"\n",
    "try:\n",
    "    # params: 向 url 中增加相关的内容\n",
    "    r = requests.get(url, params=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    # 输出 get 获取的 url 链接，可以看到键值对已经附加到了原 url 之后\n",
    "    print(r.request.url)\n",
    "    print(len(r.text))\n",
    "except:\n",
    "    print(\"爬取失败\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348f059",
   "metadata": {},
   "source": [
    "### 3.4 网络图片的爬取和存储\n",
    "- 网络图片链接的格式：http://www.example.com/picture.jpg\n",
    "- 国家地理：http://www.nationalgeographic.com.cn\n",
    "\n",
    "https://gitee.com/lockegogo/markdown_photo/raw/master/202201221702363.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4af977d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "文件保存成功\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "root = \"D://pics//\"\n",
    "url = \"https://gitee.com/lockegogo/markdown_photo/raw/master/202201221702363.png\"\n",
    "path = root + url.split('/')[-1]\n",
    "try:\n",
    "    if not os.path.exists(root):\n",
    "        os.makedirs(root)\n",
    "    if not os.path.exists(path):\n",
    "        r = requests.get(url)\n",
    "        print(r.status_code)\n",
    "        # 把图片保存为一个文件\n",
    "        with open(path, 'wb') as f:\n",
    "            f.write(r.content)  #r.content为返回内容的二进制形式\n",
    "            f.close()\n",
    "            print(\"文件保存成功\")\n",
    "    else:\n",
    "        print(\"文件已存在\")\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b30f50",
   "metadata": {},
   "source": [
    "### 3.5 IP 地址归属地的自动查询\n",
    "python 没有关于 ip 归属地的库文件。我们可以通过 ip138 进行查询。\n",
    "- https://www.ip138.com/\n",
    "- 通过搜索任意一个 ip 地址可以得知，该 url 形式为：https://www.ip138.com/iplookup.asp?ip=ipaddress&action=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c27df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cache.ip138.com/static/style/index/iplookup.css\"/>\n",
      "\t\t<script type=\"text/javascript\" src=\"//cache.ip138.com/static/script/index/iplookup-fun.js\" charset=\"utf-8\"></script>\n",
      "\t\t<script type=\"text/javascript\">\n",
      "\t\t\t(function(){\n",
      "\t\t\t\tvar u = navigator.userAgent;\n",
      "\t\t\t\tif(u.indexOf('Android') > -1 || u.indexOf('Adr') > -1){\n",
      "\t\t\t\t\tlocation.href = 'https://m.ip138.com/iplookup.asp'+location.search;\n",
      "\t\t\t\t}\n",
      "\t\t\t})();\n",
      "\t\t\tvar ip_result = {\"ASN归属地\":\"美国 加利福尼亚 洛杉矶   \", \"iP段\":\"104.225.232.0 - 104.225.239.255\", \"兼容IPv6地址\":\"::68E1:EE04\", \"映射IPv6地址\":\"::FFFF:68E1:EE04\", \"ip_c_list\":[{\"begin\":1759635456, \"end\":1759637503, \"ct\":\"美国\", \"prov\":\"加利福尼亚\", \"city\":\"洛杉矶\", \"area\":\"\", \"idc\":\"\", \"yunyin\":\"\", \"net\":\"\"}], \"zg\":1};\n",
      "\t\t\tvar ip_begin = 1759636996;\n",
      "\t\t</script>\n",
      "\t</head>\n",
      "\t<body>\n",
      "\t\t<div class=\"wrapper\">\n",
      "\t\t\t<div class=\"header\">\n",
      "\t\t\t\t<div class=\"mod-head\">\n",
      "\t\t\t\t\t<ul class=\"link only-pc\">\n",
      "\t\t\t\t\t\t<li><span class=\"icon-date\"></span></li>\n",
      "\t\t\t\t\t</ul>\n",
      "\t\t\t\t\t<a class=\"logo\" href=\"/\"><img src=\"//cache.ip138.com/sta\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "kv = {'user-agent':'Mozilla/5.0'}\n",
    "url = \"https://www.ip138.com/iplookup.asp?ip=\"\n",
    "try:\n",
    "    r = requests.get(url + '104.225.238.4' + '&action=2',headers=kv)\n",
    "    r.raise_for_status()\n",
    "    r.encoding = r.apparent_encoding\n",
    "    print(r.text[1000:2000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc0bc1",
   "metadata": {},
   "source": [
    "## 4. BeautifulSoup 库入门\n",
    "为什么学习 BeautifulSoup 库？根据前面的学习，我们已经可以做到从网站上爬取需要的内容，并且打印出来，但是局限于这些 html 标签，无法有效利用，后续操作就需要用到 BeautifulSoup，解析 html 代码或是 sml 文档。\n",
    "### 4.1 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f48c1da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head><title>This is a python demo page</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The demo python introduces several python courses.</b></p>\n",
      "<p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "<a href=\"http://www.icourse163.org/course/BIT-268001\" class=\"py1\" id=\"link1\">Basic Python</a> and <a href=\"http://www.icourse163.org/course/BIT-1001870001\" class=\"py2\" id=\"link2\">Advanced Python</a>.</p>\n",
      "</body></html>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://python123.io/ws/demo.html\")\n",
    "demo = r.text\n",
    "print(demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a792e9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   This is a python demo page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The demo python introduces several python courses.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"course\">\n",
      "   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "   <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "    Basic Python\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "    Advanced Python\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "#从 bs4 中引入一个类，即 BeautifulSoup\n",
    "from bs4 import BeautifulSoup\t\n",
    "# 除了给出 demo，还需要给出解释器：html.parser (对 demo 进行 html 的解析)\n",
    "# soup = BeautifulSoup('<p>data</p>', 'html.parser')\n",
    "soup = BeautifulSoup(demo, 'html.parser')\n",
    "print(soup.prettify())\n",
    "\n",
    "# 也可以通过提供文件的方式 \n",
    "soup2 = BeautifulSoup(open(\"D://demo.html\"), 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200b77d",
   "metadata": {},
   "source": [
    "首先源文件，即从浏览器中查看网页源代码或是通过 requests 库 get 到的 text 文件类型都如下图一般，由一对尖括号构成的标签组织起来的，每一对尖括号形成一对标签，标签之间存在上下元关系，从而形成一个标签树。\n",
    "\n",
    "![image-20220123113636188](https://gitee.com/lockegogo/markdown_photo/raw/master/202201231136253.png)\n",
    "\n",
    "- p：标签的 Name，成对出现\n",
    "- class=”title”：属性 Attributes（属性域），包含 0 个或多个属性，用以定义标签的特点，由键值对构成。\n",
    "\n",
    "Beautiful Soup 库即是解析、遍历、维护 “标签树” 的功能库，也称 beautifulsoup4 或者 bs4\n",
    "\n",
    "标签树可经 BeautifulSoup 处理转换成 BeautifulSoup 类，通常认为 html 文档，标签树，BeautifulSoup 类三者等价。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb669ed",
   "metadata": {},
   "source": [
    "### 4.2 BeautifulSoup 库的解析器\n",
    "\n",
    "|解析器|使用方法|条件|\n",
    "|----|----|----|\n",
    "|bs4 的 HTML 解析器| BeautifulSoup(mk,'html.parser')|安装 bs4 库|\t\t\t\t对于任意user-agent，都应遵循以下协议|\n",
    "|lxml 的 HTML 解析器| BeautifulSoup(mk,'lxml')|pip install lxml|\n",
    "|lxml 的 XML 解析器| BeautifulSoup(mk,'xml')|pip install lxml|\n",
    "|html5lib 的 解析器| BeautifulSoup(mk,'html5lib')|pip install html5lib|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff1af2c",
   "metadata": {},
   "source": [
    "### 4.3 BeautifulSoup 类的基本元素\n",
    "| 基本元素        | 说明                                                        |\n",
    "| --------------- | ----------------------------------------------------------- |\n",
    "| Tag             | 标签，最基本的信息组织单元，分别用 <> 和 </> 标明开头和结尾 |\n",
    "| Name            | 标签的名字，`<p>...</p>` 的名字是 ’p’，格式：`<tag>.name`    |\n",
    "| Attributes      | 标签的属性，字典形式组织，格式：`<tag>.attrs`               |\n",
    "| NavigableString | 标签内非属性字符串，<>…</> 中的字符串，格式：`<tag>.string` |\n",
    "| Comment         | 标签内字符串的注释部分，一种特殊的 Comment 类型             |\n",
    "\n",
    "标签的获取方法：\n",
    "\n",
    "<img src=\"https://gitee.com/lockegogo/markdown_photo/raw/master/202201231207198.png\" alt=\"image-20220123120708581\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f8a5423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head><title>This is a python demo page</title></head>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<title>This is a python demo page</title>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'p'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'body'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(demo, \"html.parser\")\n",
    "soup.head\n",
    "soup.title\n",
    "# 获取 a 标签：链接标签；只能返回第一个\n",
    "soup.a   \n",
    "soup.a.name   # a 标签的名称 \"a\"\n",
    "# 获取父标签的名字\n",
    "soup.a.parent.name\n",
    "soup.a.parent.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed4f1913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': 'http://www.icourse163.org/course/BIT-268001',\n",
       " 'class': ['py1'],\n",
       " 'id': 'link1'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'http://www.icourse163.org/course/BIT-268001'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Basic Python'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "bs4.element.Tag"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得标签的属性信息\n",
    "tag = soup.a\n",
    "tag.attrs\n",
    "tag.attrs['href']  # a 标签['href']属性\n",
    "print('-'*20)\n",
    "# 查看 标签 和 标签属性 的类型\n",
    "type(tag)\n",
    "type(tag.attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2062f43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Basic Python'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['Basic Python']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag.string\n",
    "tag.contents\n",
    "# tag.parent.contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f66be",
   "metadata": {},
   "source": [
    "### 4.4 `prettify()` 方法\n",
    "辅助作用，在每一个标签后加入换行符’\\n’，使得输出文本容易阅读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a4e825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--STATUS OK-->\n",
      "<html>\n",
      " <head>\n",
      "  <meta content=\"text/html;charset=utf-8\" http-equiv\n",
      "<a class=\"mnav\" href=\"http://news.baidu.com\" name=\"tj_trnews\">\n",
      " 新闻\n",
      "</a>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://www.baidu.com\"\n",
    "r = requests.get(url)\n",
    "r.encoding = r.apparent_encoding\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "tag = soup.a\n",
    "print(soup.prettify()[:100])\t\t#标签树\n",
    "print(tag.prettify())\t\t#每个标签同样适用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c4ee7",
   "metadata": {},
   "source": [
    "### 4.5 标签树的遍历\n",
    "![image-20220123123333890](https://gitee.com/lockegogo/markdown_photo/raw/master/202201231233201.png)\n",
    "| 属性         | 说明                                                     |\n",
    "| ------------ | -------------------------------------------------------- |\n",
    "| .contents    | 子节点的列表，将 `<tag>` 所有儿子节点存入列表            |\n",
    "| .children    | 子节点的迭代类型，与.contents 类似，用于循环遍历儿子节点 |\n",
    "| .descendants | 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历       |\n",
    "| .parent  | 节点的父亲标签                               |\n",
    "| .parents | 节点先辈标签的迭代类型，用于循环遍历先辈节点 |\n",
    "| .next_sibling      | 返回按照 HTML 文本顺序的下一个平行节点标签             |\n",
    "| .previous_sibling  | 返回按照 HTML 文本顺序的上一个平行节点标签             |\n",
    "| .next_siblings     | 迭代类型，返回按照 HTML 文本顺序的后续所有平行节点标签 |\n",
    "| .previous_siblings | 迭代类型，返回按照 HTML 文本顺序的前续所有平行节点标签 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4bfb435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   This is a python demo page\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The demo python introduces several python courses.\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"course\">\n",
      "   Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "   <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">\n",
      "    Basic Python\n",
      "   </a>\n",
      "   and\n",
      "   <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">\n",
      "    Advanced Python\n",
      "   </a>\n",
      "   .\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# url = 'https://baidu.com'\n",
    "# r = requests.get(url)\n",
    "# r.encoding = r.apparent_encoding\n",
    "# soup = BeautifulSoup(r.text,\"html.parser\")\n",
    "soup = BeautifulSoup(demo,\"html.parser\")\n",
    "# 返回值是列表，可以用下标来获取相关元素\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "81287ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', <p class=\"title\"><b>The demo python introduces several python courses.</b></p>, '\\n', <p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a> and <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>.</p>, '\\n']\n",
      "\n",
      "\n",
      "<p class=\"title\"><b>The demo python introduces several python courses.</b></p>\n",
      "\n",
      "\n",
      "<p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a> and <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>.</p>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 下行遍历\n",
    "# 将标签的儿子节点存入列表，.contents 返回列表类型，而 .children 返回 迭代类型\n",
    "print(soup.body.contents)\n",
    "\n",
    "for child in soup.body.children:\n",
    "\tprint(child)\n",
    "# print('-'*50)\n",
    "# for child in soup.body.descendants:\n",
    "# \tprint(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7b831477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "\n",
      "<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>\n",
      " and \n",
      "<a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "## 上行遍历\n",
    "for parent in soup.a.parent:\n",
    "    print(parent)\n",
    "    # if parent is None:\n",
    "    #     print(parent)\n",
    "    # else:\n",
    "    #     print(parent.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "52aea213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 标签的上一个兄弟标签: Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "\n",
      "a 标签的下一个兄弟标签:  and \n",
      "a 标签的下一个兄弟标签: <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>\n",
      "a 标签的下一个兄弟标签: .\n"
     ]
    }
   ],
   "source": [
    "## 平行遍历\n",
    "# 输出有可能不是标签，而是文本的形式\n",
    "for sibling in soup.a.previous_siblings:\n",
    "    print(\"a 标签的上一个兄弟标签:\", sibling)\n",
    "for sibling in soup.a.next_siblings:\n",
    "    print(\"a 标签的下一个兄弟标签:\", sibling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1b3ee",
   "metadata": {},
   "source": [
    "## 5. 信息组织与提取方法\n",
    "为什么要进行信息标记？假如给出一组信息，'北京理工大学'；'1940'；'北京市海淀区中关村'。我们需要对信息做一定的标记，来理解信息的真实含义。例如:\n",
    "- `'name': '北京理工大学'`\n",
    "- `'addr': '北京市海淀区中关村'`\n",
    "- `'year': '1940'`\n",
    "\n",
    "1. 标记后的信息可形成信息组织结构，增加了信息维度\n",
    "2. 标记后的信息可用于通信、存储或展示\n",
    "3. 标记的结构与信息一样具有重要价值\n",
    "4. 标记后的信息更利于程序理解和运用\n",
    "\n",
    "### 5.1 HTML 的信息标记\n",
    "- HTML 是 WWW（World Wide Web）的信息组织方式，能够将声音 图像 视频等超文本信息嵌入到文本中。\n",
    "- HTML 通过预定义的 <>…</> 标签形式组织不同类型的信息\n",
    "\n",
    "### 5.2 信息标记的三种形式\n",
    "#### 5.2.1 XML：扩展标记语言\n",
    "- 采用以标签为主来构建信息、表达信息的方式：`<img src=\"china.jpg\" size=\"10\">...</img>`\n",
    "- 如果标签中没有内容，则可以用空元素的缩写形式：`<img src=\"china,jpg\" size=\"10\" />`\n",
    "\n",
    "#### 5.2.2 JSON：JavaScript 中对面向对象信息的一种表达形式\n",
    "- 有类型的键值对 `key:value` 构建的信息表达方式\n",
    "- `\"name\": \"复旦大学\"`\n",
    "- `\"name\": [\"复旦大学\", \"南开大学\"]`\n",
    "- `\"name\": {\"newName\": \"北京理工大学\", \"oldName\": \"延安自然科学院\"}`\n",
    "\n",
    "#### 5.2.3 YAML：表达数据序列化的格式\n",
    "- 使用无类型键值对 `key:value`, eg. `name:北京理工大学`\n",
    "- 通过缩进的形式表达所属关系\n",
    "- 使用 `-` 表达并列关系\n",
    "- 使用 `|` 表达整块数据，信息过多时使用\n",
    "- 使用 `#` 表达注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9e18aff0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_24328/1107086886.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LOCKEG~1\\AppData\\Local\\Temp/ipykernel_24328/1107086886.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    name:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "name:\n",
    "\tnewName:北京理工大学\n",
    "\toldName:延安自然科学院\n",
    "\n",
    "name:\n",
    "-北京理工大学\n",
    "-延安自然科学院"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425013bf",
   "metadata": {},
   "source": [
    "#### 5.2.4 三种信息标记形式的比较\n",
    "- `XML `：最早的通用信息标记语言，可扩展性好，但繁琐。用于 Internet 上的信息交互与传递。\n",
    "- `JSON`：信息有类型，适合程序处理（js），较 XML 简洁。移动应用云端和节点的信息通信，无注释。\n",
    "- `YAML`：信息无类型，文本信息比例最高，可读性好。各类系统的配置文件，有注释易读。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XML 实例：\n",
    "<person>\n",
    "    <firstName>Tian</firstName>\n",
    "    <lastName>Song</lastName>\n",
    "    <address>\n",
    "        <streetAddr>中关村</streetAddr>\n",
    "        <city>北京市</city>\n",
    "        <zipcode>100081</zipcode>\n",
    "    </address>\n",
    "    <prof>Computer System</prof><prof>Security</prof>\n",
    "</person>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27d446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON 实例：\n",
    "{\n",
    "    \"firstName\"\t:\"Tian\",\n",
    "    \"lastName\"\t:\"Song\",\n",
    "    \"address\"\t:{\n",
    "        \t\t\t\"streetAddr\":\"中关村南大街5号\",\n",
    "        \t\t\t\"city\"\t\t:\"北京市\",\n",
    "        \t\t\t\"zipcode\"\t:\"100081\"\n",
    "    \t\t\t },\n",
    "    \"prof\"\t    :[\"Computer System\",\"Security\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAML 实例：\n",
    "firstName\t: Tian\n",
    "lastName\t: Song\n",
    "address\t\t:\n",
    "\tstreetAddr\t: 中关村南大街5号\n",
    "\tcity\t\t: 北京市\n",
    "\tzipcode\"\t: 100081\n",
    "prof\t    :\n",
    "-Computer System\n",
    "-Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b9abc",
   "metadata": {},
   "source": [
    "### 5.3 信息提取的一般方法\n",
    "#### 5.3.1 完整解析信息的标记形式，再提取关键信息\n",
    "- 就是用标记解析器去解析 XML JSON YAML 格式，再将其中所需要的信息提取出来\n",
    "- 例如 bs4 库提供了对标签树的遍历，需要解析什么信息对标签树遍历即可\n",
    "- 优点：信息解析准确\n",
    "- 缺点：提取过程繁琐，速度慢\n",
    "\n",
    "#### 5.3.2 无视标记形似，直接搜索关键信息\n",
    "- 对信息的文本查找函数即可\n",
    "- 优点：提取过程简洁，速度较快\n",
    "- 缺点：提取结果准确性与信息内容相关\n",
    "\n",
    "#### 5.3.3 结合使用\n",
    "- 实际使用中通常将两种方法结合起来使用，需要这就需要了**标记解析器**及**文本查找函数**。\n",
    "- 例如提取 HTML 中所有 URL 链接\n",
    "  - 搜索到所有 `<a>` 标签\n",
    "  -  解析 `<a>` 标签格式，提取 href 后的链接内容\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "13295089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.icourse163.org/course/BIT-268001\n",
      "http://www.icourse163.org/course/BIT-1001870001\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "\tprint(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06233316",
   "metadata": {},
   "source": [
    "### 5.4 基于 bs4 库的 HTML 内容查找方法\n",
    "\n",
    "- `find_all()` 方法：`<>.find_all(name,attrs,recursive,string,**kwargs)`，返回列表，存储查找结果\n",
    "  - `name`: 对标签名称的检索字符串\n",
    "  - `attrs`：对标签属性值的检索字符串，可标注属性检索\n",
    "  - `recursive`：是否对子孙全部检索，默认为 True\n",
    "  - `string`：<>…</> 中字符串区域的检索字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "26e814fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>,\n",
       " <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('a')\n",
    "print('-'*50)\n",
    "# 无输出值，说明 soup 根节点的子孙节点中含有 a 标签，但其子节点无 a 标签\n",
    "soup.find_all('a', recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46998a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The demo python introduces several python courses.</b>,\n",
       " <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>,\n",
       " <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f340ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "head\n",
      "title\n",
      "body\n",
      "p\n",
      "b\n",
      "p\n",
      "a\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "# 传入参数为 True 时，显示文档中所有标签\n",
    "for tag in soup.find_all(True):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60baacab",
   "metadata": {},
   "source": [
    "但是这种查找方式必须对属性精确赋值，如果我们想用其中一部分进行查找，就需要用到<font color=pink>正则表达式</font>。\n",
    "\n",
    "- `re.compile(pattern[, flags])`: 该函数根据包含的正则表达式的字符串创建模式对象，可以实现更有效率的匹配。\n",
    "  \n",
    "在直接使用字符串表达的正则表达式进行 `search`，`match` 和 `findall` 操作时，pythnon 会将字符串转换为 正则表达式对象，而使用 compile 完成一次转换后，在每次使用模式的时候就不用重复转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "96f937b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "# 显示所有以 b 开头的标签：正则表达式\n",
    "import re\n",
    "for tag in soup.find_all(re.compile('b')):\n",
    "    print(tag.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0e2713b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p class=\"course\">Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
       " <a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a> and <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>.</p>]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find_all(name,attrs): course 是属性值\n",
    "soup.find_all('p','course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9d978987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查找 id 属性为 link1 的元素\n",
    "soup.find_all(id = \"link1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "183bc29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"py1\" href=\"http://www.icourse163.org/course/BIT-268001\" id=\"link1\">Basic Python</a>,\n",
       " <a class=\"py2\" href=\"http://www.icourse163.org/course/BIT-1001870001\" id=\"link2\">Advanced Python</a>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# 查找 id 属性值含有 link 的元素\n",
    "soup.find_all(id = re.compile('link'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1f1c9080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basic Python']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Python\n"
     ]
    }
   ],
   "source": [
    "# 对字符串域进行查找\n",
    "soup.find_all(string = \"Basic Python\")\n",
    "\n",
    "for tag in soup.find_all(string = \"Basic Python\"):\n",
    "   print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4db1caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is a wonderful general-purpose programming language. You can learn Python from novice to professional by tracking the following courses:\n",
      "\n",
      "Basic Python\n",
      "Advanced Python\n"
     ]
    }
   ],
   "source": [
    "# 同样，利用正则表达式库可以部分检索，把含有 python 的字符串全部检索出来\n",
    "import re\n",
    "for tag in soup.find_all(string = re.compile(\"Python\")):\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21646c40",
   "metadata": {},
   "source": [
    "**`find_all()` 扩展方法：**\n",
    "\n",
    "| 方法                        | 说明                                                      |\n",
    "| --------------------------- | --------------------------------------------------------- |\n",
    "| <>.find                     | 搜索且只返回一个结果，字符串类型，同.find_all () 参数     |\n",
    "| <>.find_parents()           | 在先辈节点中搜索，返回列表类型，同.find_all () 参数       |\n",
    "| <>.find_parent()            | 在先辈节点中返回一个结果，字符串类型，同.find () 参数     |\n",
    "| <>.find_next_siblings()     | 在后续平行节点中搜索，返回列表类型，同.find_all () 参数   |\n",
    "| <>.find_next_sibling()      | 在后续平行节点中返回一个结果，字符串类型，同.find () 参数 |\n",
    "| <>.find_previous_siblings() | 在前序平行节点中搜索，返回列表类型，同.find_all () 参数   |\n",
    "| <>.find_previous_sibling()  | 在前序平行节点中返回一个结果，字符串类型，同.find () 参数 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b2f54",
   "metadata": {},
   "source": [
    "## 6. 中国大学排名定向爬虫实例\n",
    "### 6.1 准备工作\n",
    "- 输入：大学排名 URL 链接\n",
    "- 输出：大学排名信息的屏幕输出（排名、大学名称、总分）\n",
    "\n",
    "可分为三个步骤：\n",
    "1. 从网络上获取大学排名网页内容 `getHTMLText()`\n",
    "2. 提取网页内容中信息到合适的数据结构 `fillUnivList()`\n",
    "3. 利用数据结构展示并输出结果 `printUnivList()`\n",
    "\n",
    "### 6.2 爬虫编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "4668dc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '清华大学 ', '969.2'], ['2', '北京大学 ', '855.3'], ['3', '浙江大学 ', '768.7'], ['4', '上海交通大学 ', '723.4'], ['5', '南京大学 ', '654.8'], ['6', '复旦大学 ', '649.7'], ['7', '中国科学技术大学 ', '577.0'], ['8', '华中科技大学 ', '574.3'], ['9', '武汉大学 ', '567.9'], ['10', '西安交通大学 ', '537.9'], ['11', '哈尔滨工业大学 ', '522.6'], ['12', '中山大学 ', '519.3'], ['13', '北京师范大学 ', '518.3'], ['14', '四川大学 ', '516.6'], ['15', '北京航空航天大学 ', '513.8'], ['16', '同济大学 ', '508.3'], ['17', '东南大学 ', '488.1'], ['18', '中国人民大学 ', '487.8'], ['19', '北京理工大学 ', '474.0'], ['20', '南开大学 ', '465.3'], ['21', '山东大学 ', '447.0'], ['22', '天津大学 ', '444.3'], ['23', '中南大学 ', '442.2'], ['24', '吉林大学 ', '435.7'], ['25', '西北工业大学 ', '430.5'], ['26', '厦门大学 ', '427.8'], ['27', '华南理工大学 ', '419.8'], ['28', '大连理工大学 ', '418.2'], ['29', '华东师范大学 ', '401.8'], ['30', '中国农业大学 ', '400.4']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_17f34_ th {\n",
       "          text-align: left;\n",
       "    }#T_17f34_row0_col0,#T_17f34_row0_col1,#T_17f34_row0_col2,#T_17f34_row1_col0,#T_17f34_row1_col1,#T_17f34_row1_col2,#T_17f34_row2_col0,#T_17f34_row2_col1,#T_17f34_row2_col2,#T_17f34_row3_col0,#T_17f34_row3_col1,#T_17f34_row3_col2,#T_17f34_row4_col0,#T_17f34_row4_col1,#T_17f34_row4_col2{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_17f34_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >排名</th>        <th class=\"col_heading level0 col1\" >学校名称</th>        <th class=\"col_heading level0 col2\" >总分</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_17f34_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_17f34_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_17f34_row0_col1\" class=\"data row0 col1\" >清华大学 </td>\n",
       "                        <td id=\"T_17f34_row0_col2\" class=\"data row0 col2\" >969.2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_17f34_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_17f34_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "                        <td id=\"T_17f34_row1_col1\" class=\"data row1 col1\" >北京大学 </td>\n",
       "                        <td id=\"T_17f34_row1_col2\" class=\"data row1 col2\" >855.3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_17f34_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_17f34_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "                        <td id=\"T_17f34_row2_col1\" class=\"data row2 col1\" >浙江大学 </td>\n",
       "                        <td id=\"T_17f34_row2_col2\" class=\"data row2 col2\" >768.7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_17f34_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_17f34_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "                        <td id=\"T_17f34_row3_col1\" class=\"data row3 col1\" >上海交通大学 </td>\n",
       "                        <td id=\"T_17f34_row3_col2\" class=\"data row3 col2\" >723.4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_17f34_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_17f34_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "                        <td id=\"T_17f34_row4_col1\" class=\"data row4 col1\" >南京大学 </td>\n",
       "                        <td id=\"T_17f34_row4_col2\" class=\"data row4 col2\" >654.8</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2663a4e56a0>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_list = []\n",
    "url = \"https://www.shanghairanking.cn/rankings/bcur/2021\"\n",
    "r = requests.get(url, timeout=30)\n",
    "r.raise_for_status()\n",
    "# 修改编码\n",
    "r.encoding = r.apparent_encoding\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "for tr in soup.find('tbody').children:\n",
    "    # 通过检查网页源代码，发现这里的每一个 tr 标签都包裹了一个大学的信息\n",
    "    # 检测 tr 标签的类型，如果不是 bs4 定义的 Tag 会过滤掉\n",
    "    # isinstance() 函数对变量的类型作出判断\n",
    "    if isinstance(tr, bs4.element.Tag):\n",
    "        tds = tr('td')  # 将筛选后的 td 标签加入列表中，这里省略了 tr.find_all('td')\n",
    "        # print(tds[0].string.strip())\n",
    "        univ = tr.find('a')  # 找到大学名称标签\n",
    "        # print(univ.string)\n",
    "        uni_list.append(\n",
    "            [tds[0].string.strip(), univ.string, tds[4].string.strip()])\n",
    "# 只能提取当前页面的 30 个，想翻页怎么办？\n",
    "print(uni_list)\n",
    "\n",
    "table = pd.DataFrame()\n",
    "table = table.append(uni_list[:5])\n",
    "list = ['排名', '学校名称', '总分']\n",
    "table.columns = list\n",
    "# 设置表格左对齐\n",
    "table = table.style.set_properties(**{'text-align': 'left'})\n",
    "table.set_table_styles(\n",
    "    [dict(selector='th', props=[('text-align', 'left')])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af185f",
   "metadata": {},
   "source": [
    "接下来我们尝试用三个函数来对上述代码进行重构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6b206e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_ebae9_ th {\n",
       "          text-align: left;\n",
       "    }#T_ebae9_row0_col0,#T_ebae9_row0_col1,#T_ebae9_row0_col2,#T_ebae9_row1_col0,#T_ebae9_row1_col1,#T_ebae9_row1_col2,#T_ebae9_row2_col0,#T_ebae9_row2_col1,#T_ebae9_row2_col2,#T_ebae9_row3_col0,#T_ebae9_row3_col1,#T_ebae9_row3_col2,#T_ebae9_row4_col0,#T_ebae9_row4_col1,#T_ebae9_row4_col2,#T_ebae9_row5_col0,#T_ebae9_row5_col1,#T_ebae9_row5_col2,#T_ebae9_row6_col0,#T_ebae9_row6_col1,#T_ebae9_row6_col2,#T_ebae9_row7_col0,#T_ebae9_row7_col1,#T_ebae9_row7_col2,#T_ebae9_row8_col0,#T_ebae9_row8_col1,#T_ebae9_row8_col2,#T_ebae9_row9_col0,#T_ebae9_row9_col1,#T_ebae9_row9_col2{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_ebae9_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >排名</th>        <th class=\"col_heading level0 col1\" >学校名称</th>        <th class=\"col_heading level0 col2\" >总分</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ebae9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_ebae9_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "                        <td id=\"T_ebae9_row0_col1\" class=\"data row0 col1\" >清华大学 </td>\n",
       "                        <td id=\"T_ebae9_row0_col2\" class=\"data row0 col2\" >969.2</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_ebae9_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "                        <td id=\"T_ebae9_row1_col1\" class=\"data row1 col1\" >北京大学 </td>\n",
       "                        <td id=\"T_ebae9_row1_col2\" class=\"data row1 col2\" >855.3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_ebae9_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "                        <td id=\"T_ebae9_row2_col1\" class=\"data row2 col1\" >浙江大学 </td>\n",
       "                        <td id=\"T_ebae9_row2_col2\" class=\"data row2 col2\" >768.7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_ebae9_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "                        <td id=\"T_ebae9_row3_col1\" class=\"data row3 col1\" >上海交通大学 </td>\n",
       "                        <td id=\"T_ebae9_row3_col2\" class=\"data row3 col2\" >723.4</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_ebae9_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "                        <td id=\"T_ebae9_row4_col1\" class=\"data row4 col1\" >南京大学 </td>\n",
       "                        <td id=\"T_ebae9_row4_col2\" class=\"data row4 col2\" >654.8</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_ebae9_row5_col0\" class=\"data row5 col0\" >6</td>\n",
       "                        <td id=\"T_ebae9_row5_col1\" class=\"data row5 col1\" >复旦大学 </td>\n",
       "                        <td id=\"T_ebae9_row5_col2\" class=\"data row5 col2\" >649.7</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_ebae9_row6_col0\" class=\"data row6 col0\" >7</td>\n",
       "                        <td id=\"T_ebae9_row6_col1\" class=\"data row6 col1\" >中国科学技术大学 </td>\n",
       "                        <td id=\"T_ebae9_row6_col2\" class=\"data row6 col2\" >577.0</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_ebae9_row7_col0\" class=\"data row7 col0\" >8</td>\n",
       "                        <td id=\"T_ebae9_row7_col1\" class=\"data row7 col1\" >华中科技大学 </td>\n",
       "                        <td id=\"T_ebae9_row7_col2\" class=\"data row7 col2\" >574.3</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_ebae9_row8_col0\" class=\"data row8 col0\" >9</td>\n",
       "                        <td id=\"T_ebae9_row8_col1\" class=\"data row8 col1\" >武汉大学 </td>\n",
       "                        <td id=\"T_ebae9_row8_col2\" class=\"data row8 col2\" >567.9</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ebae9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_ebae9_row9_col0\" class=\"data row9 col0\" >10</td>\n",
       "                        <td id=\"T_ebae9_row9_col1\" class=\"data row9 col1\" >西安交通大学 </td>\n",
       "                        <td id=\"T_ebae9_row9_col2\" class=\"data row9 col2\" >537.9</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2663ab18df0>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def getHTMLText(url):\n",
    "    \"\"\"\n",
    "    从网络上获取大学排名网页内容\n",
    "    \"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"爬取失败\"\n",
    "\n",
    "\n",
    "def fillUnivList(html):\n",
    "    \"\"\"\n",
    "    提取网页内容中信息到合适的数据结构\n",
    "    \"\"\"\n",
    "    uni_list = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tr in soup.find('tbody').children:\n",
    "        if isinstance(tr, bs4.element.Tag):\n",
    "            tds = tr('td')  # 将筛选后的 td 标签加入列表中\n",
    "            univ = tr.find('a')  # 找到大学名称标签\n",
    "            uni_list.append(\n",
    "                [tds[0].string.strip(), univ.string, tds[4].string.strip()])\n",
    "    return uni_list\n",
    "\n",
    "\n",
    "def printUnivList(uni_list, num):\n",
    "    \"\"\"\n",
    "    利用数据结构展示并输出结果\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append(uni_list[:num])\n",
    "    list = ['排名', '学校名称', '总分']\n",
    "    table.columns = list\n",
    "    # 设置表格左对齐\n",
    "    table = table.style.set_properties(**{'text-align': 'left'})\n",
    "    table.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])])\n",
    "    return table\n",
    "    return table\n",
    "\n",
    "\n",
    "# prevent parts of code from being run when the modules are imported.\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.shanghairanking.cn/rankings/bcur/2021\"\n",
    "    html = getHTMLText(url)\n",
    "    uni_list = fillUnivList(html)\n",
    "    table = printUnivList(uni_list, 10)  # 前 20 名\n",
    "    table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669f03a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ea81a6e",
   "metadata": {},
   "source": [
    "## 7. Re 库：正则表达式库\n",
    "`regular expression`: 通用的字符串表达框架，用来简洁表达一组字符串的表达式。\n",
    "\n",
    "re 库采用 <font color=pink>raw string</font>  类型 (原生字符串类型，不包含转义符) 表示正则表达式，表示为：`r'text'`，例如邮政编码：`r'[1-9]\\d{5}'`，这里的 \\ 就不包含转义，如果用 string 类型需要额外添加一个 \\\n",
    "\n",
    "在文本处理中，可用于表达文本类型的特征，同时查找或替换一组字符串，匹配字符串的全部或部分。\n",
    "### 7.1 Re 库的常用操作符\n",
    "\n",
    "| 操作符 | 说明                               | 实例                                           |\n",
    "| :----- | :--------------------------------- | :--------------------------------------------- |\n",
    "| .      | 表示任何单个字符                   |                                                |\n",
    "| []     | 字符集，对单个字符给出取值范围     | [abc] 表示 a、b、c，[a-z] 表示 a 到 z 单个字符 |\n",
    "| [^]    | 非字符集，对单个字符给出排除范围   | [^abc] 表示非 a 或 b 或 c 的单个字符           |\n",
    "| *      | 前一个字符 0 次或无限次扩展        | abc * 表示 ab、abc、abcc、abccc 等             |\n",
    "| +      | 前一个字符 1 次或无限次扩展        | abc + 表示 abc、abcc、abccc 等                 |\n",
    "| ?      | 前一个字符 0 次或 1 次扩展         | abc？表示 ab、abc                              |\n",
    "| \\|     | 左右表达式任意一个                 | abc\\|def 表示 abc、def                         |\n",
    "| {m}    | 扩展前一个字符 m 次                | ab {2} c 表示 abbc                             |\n",
    "| {m,n}  | 扩展前一个字符 m 至 n 次（含 n）   | ab {1,2} c 表示 abc、abbc                      |\n",
    "| ^      | 匹配字符串开头                     | ^abc 表示 abc 且在一个字符串的开头             |\n",
    "| $      | 匹配字符串结尾                     | abc$ 表示 abc 且在一个字符串的结尾             |\n",
    "| ()     | 分组标记，内部只能使用 “\\|” 操作符 | (abc) 表示 abc，(abc\\|def)表示 abc、def     |\n",
    "| \\d     | 数字，等价于 [0-9]                 |                                                |\n",
    "| \\w     | 单词字符，等价于 [A-Za-z0-9_]      |                                                |\n",
    "\n",
    "### 7.2 Re 库的主要功能函数\n",
    "| 函数          | 说明                                                         |\n",
    "| :------------ | :----------------------------------------------------------- |\n",
    "| re.search()   | 在一个字符串中搜索匹配正则表达式的第一个位置，返回 match 对象 |\n",
    "| re.match()    | 从一个字符串的开始位置起匹配正则表达式，返回 match 对象      |\n",
    "| re.findall()  | 搜索字符串，以列表类型返回全部能匹配的子串                   |\n",
    "| re.split()    | 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型     |\n",
    "| re.finditer() | 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是 match 对象 |\n",
    "| re.sub()      | 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串 |\n",
    "\n",
    "### 7.3 match 对象\n",
    "#### 7.3.1 match 对象属性\n",
    "`re` 库的 `search`，`match`，`finditer` 函数都会返回 `match` 对象，`match` 对象就是一次匹配的结果，包含很多相关信息。\n",
    "\n",
    "可以通过 `type(match)` 查看属性，介绍以下四种:\n",
    "\n",
    "| 属性    | 说明                                    |\n",
    "| ------- | --------------------------------------- |\n",
    "| .string | 待匹配的文本                            |\n",
    "| .re     | 匹配时使用的 pattern 对象（正则表达式） |\n",
    "| .pos    | 正则表达式搜索文本的开始位置            |\n",
    "| .endpos | 正则表达式搜索文本的结束位置            |\n",
    "\n",
    "#### 7.3.2 match 对象的方法\n",
    "| 方法      | 说明                             |\n",
    "| --------- | -------------------------------- |\n",
    "| .group(0) | 获得匹配后的字符串               |\n",
    "| .start()  | 匹配字符串在原始字符串的开始位置 |\n",
    "| .end()      | 匹配字符串在原始字符串的结束位置 |\n",
    "| .span()     | 返回 (.start (),.end ())         |\n",
    "\n",
    "\n",
    "```python\n",
    "re.search(pattern,string,flags=0)\n",
    "```\n",
    "- `pattern`: 正则表达式的字符串或原生字符串表示\n",
    "- `string`: 待匹配字符串\n",
    "- `flags`: 正则表达式使用时的控制标记\n",
    "\n",
    "| 常用标记           | 说明                                                         |\n",
    "| ------------------ | ------------------------------------------------------------ |\n",
    "| re.I re.IGNORECASE | 忽略正则表达式的大小写，[A-Z] 能苟匹配小写字符               |\n",
    "| re.M re.MULTILINE  | 正则表达式中 ^ 操作符能够将给定字符串的每行当做匹配开始      |\n",
    "| re.S re.DOTALL     | 正则表达式中的 . 操作符能够匹配所有字符，默认匹配除换行外的所有字符 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d15c86f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BIT 100081'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "re.compile(r'[1-9]\\d{5}', re.UNICODE)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'100081'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search() 函数\n",
    "import re\n",
    "match = re.search(r'[1-9]\\d{5}', 'BIT 100081')\n",
    "if match:\n",
    "    print(match.group(0))\n",
    "\n",
    "match.string\n",
    "match.re\n",
    "match.pos\n",
    "match.endpos\n",
    "print('-'*20)\n",
    "match.group(0)\n",
    "match.start()\n",
    "match.end()\n",
    "match.span()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e303d60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.Match"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a8806",
   "metadata": {},
   "source": [
    "```python\n",
    "re.match(pattern,string,flags=0)\n",
    "```\n",
    "- match 对象只能返回<font color=pink>第一次匹配</font>的结果\n",
    "- match 函数从给定字符串的头位置起匹配，如果不符合就会返回空\n",
    "- search 函数是完整地搜索给定字符串，注意和 match 区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8661883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "match = re.match(r'[1-9]\\d{5}', '100081 BIT')\n",
    "if match:\n",
    "    print(match.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a3176",
   "metadata": {},
   "source": [
    "```python\n",
    "re.findall(pattern,string,flags=0)\n",
    "```\n",
    "- 搜索字符串，以列表类型返回全部能匹配的子串\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0edd5777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100081', '100084']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ls = re.findall(r'[1-9]\\d{5}', 'BIT100081 TSU100084')\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1fff6f",
   "metadata": {},
   "source": [
    "```python\n",
    "re.split(pattern,string,maxsplit=0,flags=0)\n",
    "```\n",
    "- 将一个字符串按照正则表达式匹配结果进行分割，返回列表类型\n",
    "- `maxsplit`：最大分割数，剩余部分作为最后一个元素输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "52497276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BIT', ' TSU', '']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['BIT', ' TSU100084']"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "ls = re.split(r'[1-9]\\d{5}', 'BIT100081 TSU100084')\n",
    "ls\n",
    "print('-'*30)\n",
    "ls = re.split(r'[1-9]\\d{5}', 'BIT100081 TSU100084', maxsplit = 1)\n",
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc82208",
   "metadata": {},
   "source": [
    "```python\n",
    "re.finditer(pattern,string,flags=0)\n",
    "```\n",
    "- 搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是 match 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5534fa7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100081\n",
      "100084\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for m in re.finditer(r'[1-9]\\d{5}', 'BIT100081 TSU100084'):\n",
    "    if m:\n",
    "        print(m.group(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75315fc4",
   "metadata": {},
   "source": [
    "```python\n",
    "re.sub(pattern,repl,string,count=0,flags=0)\n",
    "```\n",
    "- 在一个字符串中替换所有匹配正则表达式的子串，返回替换后的字符串\n",
    "- `repl`: 替换匹配字符串的字符串\n",
    "- `count`: 匹配的最大替换次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "04f554d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BIT:zipcode TSU:zipcode'"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'[1-9]\\d{5}', ':zipcode', 'BIT100081 TSU100084')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b9d5c",
   "metadata": {},
   "source": [
    "### 7.4 Re 库的另一种等价用法\n",
    "**函数式用法**：一次性操作\n",
    "```python\n",
    "rst = re.search(r'[1-9]\\d{5}', 'BIT100081')\n",
    "```\n",
    "**面向对象用法**：一次编译，多次操作\n",
    "\n",
    "```python\n",
    "pat = re.compile(r'[1-9]\\d{5}')\n",
    "rst = pat.search('BIT100081')\n",
    "```\n",
    "注意：将原生字符串编译之后的正则对象，才是真正意义上的字符串，才可以和给定的字符串进行比较。\n",
    "\n",
    "**补充 compile 函数**: 将正则表达式的字符串形式编译成正则表达式对象\n",
    "```python\n",
    "re.compile(pattern,flags=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacd6d0",
   "metadata": {},
   "source": [
    "### 7.5 Re 库的贪婪匹配和最小匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b63a4f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYANBNCNDN'"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.search(r'PY.*N', 'PYANBNCNDN')\n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c4129",
   "metadata": {},
   "source": [
    "从输出结果可以看到，如果同时匹配长短不同的多项，Re 库采用贪婪匹配，即输出匹配最长的子串 ’PYANBNCNDN’。\n",
    "\n",
    "那如何输出最短的子串？\n",
    "\n",
    "只需将正则表达式修改为 `r'PY.*?N'`\n",
    "\n",
    "具体介绍最小匹配操作符：\n",
    "| 操作符 | 说明                                       |\n",
    "| ------ | ------------------------------------------ |\n",
    "| *?     | 前一个字符 0 次或无限次扩展，最小匹配      |\n",
    "| +?     | 前一个字符 1 次或无限次扩展，最小匹配      |\n",
    "| ??     | 前一个字符 0 次或 1 次扩展，最小匹配       |\n",
    "| {m,n}? | 扩展前一个字符 m 至 n 次（含 n），最小匹配 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "ba837047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PYAN'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match = re.search(r'PY.*?N', 'PYANBNCNDN')\n",
    "match.group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19398d1a",
   "metadata": {},
   "source": [
    "## 8. 淘宝商品信息定向爬虫实例\n",
    "### 8.1 准备工作\n",
    "- 目标：获取淘宝搜索页面的信息，提取其中的商品名称和价格\n",
    "- 难点：淘宝的搜索接口、翻页的处理\n",
    "- 工具：`requests-re`\n",
    "\n",
    "在淘宝搜索书包，起始页 URL 链接：https://s.taobao.com/search?q=电脑&imgfile=&commend=all&ssid=s5-e&search_type=item&sourceId=tb.index&spm=2685j.jianhua.201856-taobao-item.1&ie=utf8&initiative_id=tbindexz_20170306\n",
    "\n",
    "第二页：https://s.taobao.com/search?q=电脑&imgfile=&commend=all&ssid=s5-e&search_type=item&sourceId=tb.index&spm=2685j.jianhua.201856-taobao-item.1&ie=utf8&initiative_id=tbindexz_20170306&bcoffset=1&ntoffset=1&p4ppushleft=2%2C48&s=44\n",
    "\n",
    "第三页：https://s.taobao.com/search?q=电脑&imgfile=&commend=all&ssid=s5-e&search_type=item&sourceId=tb.index&spm=2685j.jianhua.201856-taobao-item.1&ie=utf8&initiative_id=tbindexz_20170306&bcoffset=-2&ntoffset=-2&p4ppushleft=2%2C48&s=88\n",
    "\n",
    "分析出 q 后面是关键词，s 后面为页面标签。\n",
    "\n",
    "可分为三个步骤：\n",
    "1. 提交商品搜索请求，循环获取页面 `getHTMLText()`\n",
    "2. 对于每个页面，提交商品名称和价格信息 `parsepage()`\n",
    "3. 将信息输出 `printGoodsList()`\n",
    "\n",
    "该实例的主要问题就是 cookie 如何获取。\n",
    "\n",
    "cookies 是保存在浏览器中记录我们信息的，但 HTTP 协议是一种无状态协议，在数据交换完毕后，服务端和客户端的链接就会关闭，每次交换数据都需要建立新的链接。浏览器开启到关闭就是一次会话，当关闭浏览器时，会话 cookie 就会跟随浏览器而销毁。\n",
    "\n",
    "淘宝的 cookies 的查看方式：输入自己的账号密码登录，来到淘宝网首页，之后按 F12 检查元素，然后按下图所示。按下 F12 之后有可能是一片空白，此时需要再次刷新一下当前网页即可，然后，按下图所示查看 Doc 文档，你可以把所有的 Doc 文档都查看一下看看有没有 cookie，然后把它复制到 Python 代码中即可。\n",
    "\n",
    "### 8.2 爬虫编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8b941504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "访问成功\n",
      "getdata success\n",
      "访问成功\n",
      "getdata success\n",
      "访问成功\n",
      "getdata success\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_3a2c2_ th {\n",
       "          text-align: left;\n",
       "    }#T_3a2c2_row0_col0,#T_3a2c2_row0_col1,#T_3a2c2_row1_col0,#T_3a2c2_row1_col1,#T_3a2c2_row2_col0,#T_3a2c2_row2_col1,#T_3a2c2_row3_col0,#T_3a2c2_row3_col1,#T_3a2c2_row4_col0,#T_3a2c2_row4_col1{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_3a2c2_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >商品</th>        <th class=\"col_heading level0 col1\" >价格（单位：元）</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3a2c2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_3a2c2_row0_col0\" class=\"data row0 col0\" >华为笔记本电脑MateBook14 轻薄便携笔记本电脑 华为轻薄本</td>\n",
       "                        <td id=\"T_3a2c2_row0_col1\" class=\"data row0 col1\" >5699.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3a2c2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_3a2c2_row1_col0\" class=\"data row1 col0\" >电竞本拯救者Y9000P 英特尔酷睿</td>\n",
       "                        <td id=\"T_3a2c2_row1_col1\" class=\"data row1 col1\" >9899.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3a2c2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_3a2c2_row2_col0\" class=\"data row2 col0\" >店长推荐HP/惠普星青春版14 锐龙笔记本电脑</td>\n",
       "                        <td id=\"T_3a2c2_row2_col1\" class=\"data row2 col1\" >3699.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3a2c2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_3a2c2_row3_col0\" class=\"data row3 col0\" >【店铺爆款】联想小新Air14 英特尔酷睿i5 14英寸全面屏高性能商务办公便携轻薄本学生笔记本电脑 官方旗舰店</td>\n",
       "                        <td id=\"T_3a2c2_row3_col1\" class=\"data row3 col1\" >5099.00</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3a2c2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_3a2c2_row4_col0\" class=\"data row4 col0\" >ThinkPad X1 Nano 美版P15 P17 P1X1隐士 X1CarbonG9 T15GT14P14S</td>\n",
       "                        <td id=\"T_3a2c2_row4_col1\" class=\"data row4 col1\" >10999.00</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26639e48df0>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "\n",
    "def getHTMLText(url):\n",
    "    \"\"\"\n",
    "    获取 HTML 页面\n",
    "    \"\"\"\n",
    "    kv = {'User_Agent': 'Mozilla/5.0'}\n",
    "    # cookies具有有效期！！！\n",
    "    my_cookies = 'cna=PbzFGbzicFsCAWVdbS1c8F2Q; t=f3fbaa6368deca8340f2c2c7de7a6324; miid=4663826611876598161; tracknick=lk%5Cu6D1B%5Cu514B%5Cu738B; thw=cn; _m_h5_tk=2d1fcbc6447a0f6314dc197627ca1b97_1642939666517; _m_h5_tk_enc=474676ad256734fc8019058c356647b5; _samesite_flag_=true; cookie2=167dc8c0f01efb226e6d7726318171fe; _tb_token_=eb6e1e0db37be; sgcookie=E100BMyHi0mEfK02W9gXoSLt9OAJQdK%2BlOw0wUnEpgJqx6VkfUvgGBMxM2JeE8wwcBP7Txm4CQDCKg6sZWgvdp%2FQOUR27BUhzq4beVg8l5Gj2VHJlF6786jDqW%2FoxyHVH93i; unb=2262605424; uc3=id2=UUpninkmo02BYQ%3D%3D&vt3=F8dCvU14BUG%2FkjJ%2FoXw%3D&nk2=D8hWAT8Y7ro%3D&lg2=U%2BGCWk%2F75gdr5Q%3D%3D; csg=4691c27a; lgc=lk%5Cu6D1B%5Cu514B%5Cu738B; cancelledSubSites=empty; cookie17=UUpninkmo02BYQ%3D%3D; dnk=lk%5Cu6D1B%5Cu514B%5Cu738B; skt=0aa31e605c669ba6; existShop=MTY0MjkzMDAyOQ%3D%3D; uc4=nk4=0%40DeufD8Z34X%2B62bkM5nlRNsaNAA%3D%3D&id4=0%40U2gtG%2BDRUatiDBKhrY4p0LW7SznQ; _cc_=W5iHLLyFfA%3D%3D; _l_g_=Ug%3D%3D; sg=%E7%8E%8B4c; _nk_=lk%5Cu6D1B%5Cu514B%5Cu738B; cookie1=VAn7AhjuAghig2bFrVwdkC%2BR920ayz%2BcKdqNTSNBZfo%3D; enc=S%2BY6frUCv86NWs1Udb87wEPAYmrciV9%2F2ZLmG20rP6yQhm224So1tUA1VHZkhTdLfF8T1OvMPLbJXP3JBSZM1A%3D%3D; mt=ci=62_1; v=0; uc1=pas=0&cookie21=WqG3DMC9Fb5mPLIQo9kR&existShop=false&cookie15=VT5L2FSpMGV7TQ%3D%3D&cookie14=UoewAjeFukbEYQ%3D%3D&cookie16=Vq8l%2BKCLySLZMFWHxqs8fwqnEw%3D%3D; isg=BFZW-tICK8w29R9o72GNFWnKpwxY95oxn4HIUcC_VznUg_YdKIOvQbvxHx9vK5JJ; l=eBPahZvqg7F4ImULBOfanurza77OSIRYYuPzaNbMiOCPO31B5oKfW6KBZHT6C3GVh6lpR3lFyc9kBeYBq7VonxvtIosM_Ckmn'\n",
    "    my_cookies = my_cookies.split(';')\n",
    "    cookies = {}\n",
    "    for cookie in my_cookies:\n",
    "        name, value = cookie.strip().split('=', 1)\n",
    "        cookies[name] = value\n",
    "    try:\n",
    "        r = requests.get(url, headers=kv, cookies=cookies, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        r.encoding = r.apparent_encoding\n",
    "        print(\"访问成功\")\n",
    "        return r.text\n",
    "    except:\n",
    "        print(\"访问失败\")\n",
    "        return''\n",
    "\n",
    "\n",
    "def parsepage(data, html):\n",
    "    \"\"\"\n",
    "    从 HTML 页面中获取商品的名称和价格\n",
    "    \"\"\"\n",
    "    # 用 try & except 的结构消灭所有异常情况\n",
    "    try:\n",
    "        # 在页面邮件打开源代码，搜索 5099.00 | 联想小新，定位目标元素位置\n",
    "        # \"view_price\":\"5099.00\"\n",
    "        # \"raw_title\":\"联想小新\"\n",
    "        # \\d: [0-9] 在 0 至 9 上取值；\n",
    "        # \\.: 注意这里的点代表小数点而不是正则表达式中的匹配任意字符的点因为有转义符\n",
    "        # []: 限定这里的取值只能是数字或者小数点\n",
    "        # []*：* 表示前面的字符可以扩展 0 次至无数次，在本例中，扩展了 7 次\n",
    "        plt = re.findall(r'\\\"view_price\\\"\\:\\\"[\\d\\.]*\\\"', html)  # 商品价格的正则表达式\n",
    "        # .: 表示任何单个字符\n",
    "        # ?：表示前面字符 0 次或 1 次扩展\n",
    "        # *?: 最小匹配操作符\n",
    "        tlt = re.findall(r'\\\"raw_title\\\"\\:\\\".*?\\\"', html)  # 商品名称的正则表达式\n",
    "        # print(plt)\n",
    "        for i in range(len(plt)):\n",
    "            # eval 函数可以将字符串的最外层的双引号和单引号去掉\n",
    "            price = eval(plt[i].split(':')[1])\n",
    "            title = eval(tlt[i].split(':')[1])\n",
    "            data.append([title, price])\n",
    "        print('getdata success')\n",
    "    except:\n",
    "        print(\"getdata fail\")\n",
    "\n",
    "\n",
    "def printGoodsList(data, num):\n",
    "    \"\"\"\n",
    "    利用数据结构展示并输出结果\n",
    "    \"\"\"\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append(data[:num])\n",
    "    list = ['商品', '价格（单位：元）']\n",
    "    table.columns = list\n",
    "    # 设置表格左对齐\n",
    "    table = table.style.set_properties(**{'text-align': 'left'})\n",
    "    table.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])])\n",
    "    return table\n",
    "\n",
    "\n",
    "keyword = input('请输入爬取的商品的名字：')\n",
    "# 设置翻页数\n",
    "depth = 3\n",
    "start_url = 'https://s.taobao.com/search?q='+keyword\n",
    "infolist = []\n",
    "for i in range(depth):\n",
    "    try:\n",
    "        # 通过翻页观察页面变化得到的规律：页数 * 44；一个页面所展示的商品刚好就是 44\n",
    "        url = start_url + \"&s=\" + str(i*44)\n",
    "        html = getHTMLText(url)\n",
    "        parsepage(infolist, html)\n",
    "    except:\n",
    "        print('warning')\n",
    "        # 如果某一个页面出现了问题，我们继续下一个页面的解析而不影响整个程序的执行\n",
    "        continue\n",
    "# 如果爬取数目设置的很大但是翻页数很小，最终爬取出来的项目可能达不到想要的数量\n",
    "table = printGoodsList(infolist, 5)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e0c76",
   "metadata": {},
   "source": [
    "## 9. 股票数据定向爬虫\n",
    "### 9.1 准备工作\n",
    "- 目标：获取上交所和深交所所有股票的名称和交易信息\n",
    "- 输出：保存到文件中\n",
    "- 工具：requests-bs4-re\n",
    "\n",
    "股票信息网站：\n",
    "- 新浪股票：http://finance.sina.com.cn/stock/\n",
    "- 选取原则：股票信息静止存在于 HTML 界面中，非 js 代码生成，Robots 协议限制\n",
    "- 在新浪股票中查看源代码，发现价格并不在源代码中，很可能是 js 代码生成的，换别的网站试试。\n",
    "- 中财网：http://quote.cfi.cn/stockList.aspx\n",
    "\n",
    "可以分为三个步骤：\n",
    "1. 获取股票列表 getStockList()\n",
    "2. 根据股票列表获取个股信息 getStockInfo()\n",
    "3. 将结果存储到文件 \n",
    "\n",
    "### 9.2 爬虫编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520281e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import traceback\n",
    "\n",
    "kv = {'user-agent': 'Mozilla/5.0'}\n",
    "\n",
    "\n",
    "def getHTMLText(url, code='utf-8'):\n",
    "    try:\n",
    "        r = requests.get(url, headers=kv, timeout=30)\n",
    "        r.raise_for_status()\n",
    "        # r.encoding = r.apparent_encoding\n",
    "        # 直接赋值，节省时间\n",
    "        r.encoding = code\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def getStockList(lst, stockURL):\n",
    "    \"\"\"\n",
    "    获得股票列表\n",
    "    \"\"\"\n",
    "    html = getHTMLText(stockURL)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    a = soup.find_all('a')\n",
    "    for i in a:\n",
    "        try:\n",
    "            href = i.attrs['href']\n",
    "            lst.append(href)\n",
    "            # lst.append(re.findall(r\"[S][HZ]\\d{6}\", href)[0])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "def getStockInfo(lst, stockURL, fpath):\n",
    "    \"\"\"\n",
    "    根据股票列表到相关网站上获取股票信息并存储到文件中\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for stock in lst:\n",
    "        url = stockURL + stock\n",
    "        html = getHTMLText(url)\n",
    "        try:\n",
    "            # 判断是否为空页面\n",
    "            if html == \"\":\n",
    "                continue\n",
    "            infoDict = {}\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            name = soup.find('div', attrs={'class': 'Lfont'})\n",
    "            infoDict.update({'股票名称': name.text})\n",
    "            price = soup.find('span', attrs={'id': 'last'})\n",
    "            infoDict.update({'股票价格': price.text})\n",
    "            stockInfo = soup.find('table', attrs={'id': 'quotetab_stock'})\n",
    "            # 找到所有的 td 标签：股票信息，包括今开，最高之类\n",
    "            list = stockInfo.find_all('td')\n",
    "            for i in range(len(list)-1):\n",
    "                key = list[i].text.split(':')[0]\n",
    "                value = list[i].text.split(':')[1]\n",
    "                infoDict[key] = value\n",
    "            with open(fpath, 'a', encoding='utf-8') as f:\n",
    "                f.write(str(infoDict) + '\\n')\n",
    "                count = count + 1\n",
    "                # 黑科技：实现不换行的进度条 \\r\n",
    "                print('\\r当前完成进度：{:.2f}%'.format(\n",
    "                    count * 100 / len(lst)), end=\" \")\n",
    "        except:\n",
    "            traceback.print_exc()\n",
    "            count = count + 1\n",
    "            print('\\r当前完成进度：{:.2f}%'.format(count * 100 / len(lst)), end=\" \")\n",
    "            continue\n",
    "        # 全部存取太耗时了，只演示下\n",
    "        break\n",
    "\n",
    "\n",
    "def main():\n",
    "    stock_list_url = 'http://quote.cfi.cn/stockList.aspx'\n",
    "    stock_info_url = 'https://quote.cfi.cn/'\n",
    "    output_file = 'D://StockInfo.txt'\n",
    "    slist = []\n",
    "    getStockList(slist, stock_list_url)\n",
    "    getStockInfo(slist, stock_info_url, output_file)\n",
    "\n",
    "# 昨天还能跑，今天就不能跑了？\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5810df91",
   "metadata": {},
   "source": [
    "## 10. Scrapy 爬虫框架\n",
    "爬虫框架：\n",
    "1. 爬虫框架是实现爬虫功能的一个软件结构和功能组件集合\n",
    "2. 爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫\n",
    "\n",
    "<img src=\"https://gitee.com/lockegogo/markdown_photo/raw/master/202201241447115.png\" alt=\"image-20220124144734951\" style=\"zoom:67%;\" />\n",
    "\n",
    "**Engine**：\n",
    "- 控制所有模块之间的数据流\n",
    "- 根据条件触发事件\n",
    "- 不需要用户修改\n",
    "\n",
    "**Download**：\n",
    "- 根据请求下载网页\n",
    "- 不需要用户修改\n",
    "\n",
    "**Scheduler**：\n",
    "- 对所有爬取请求进行调度管理\n",
    "- 不需要用户修改\n",
    "- 因为这三个模块不需要用户手动配置，Scrapy 添加了一个中间件 Downloader Middleware，可对这三个模块进行用户可配置的控制。\n",
    "- 用户可通过编写该中间件进行修改、丢弃、新增请求或响应。\n",
    "\n",
    "**Spider（核心）**：入口\n",
    "- 解析 Downloader 返回的响应（Response）\n",
    "- 产生爬取项（scrapyed item）\n",
    "- 产生额外的爬取请求（Request）\n",
    "- 在 spider 与 engine 之间有个 Spider Middleware，用以对请求和爬取项的再处理。\n",
    "- 用户可通过编写该中间件进行修改、丢弃、新增请求或爬取项。\n",
    "\n",
    "**Item Pipelines**：出口\n",
    "- 以流水线方式处理 Spider 产生的爬取项\n",
    "- 由一组操作顺序组成，类似流水线，每个操作是一个 Item Pipeline 类型\n",
    "- 可能操作包括：清理、检验和查重爬取项中的 HTML 数据、将数据存储到数据库\n",
    "\n",
    "### 10.1 常用命令\n",
    "`>scrapy<command>[options][args]`\n",
    "| 命令             | 说明                | 格式                                        |\n",
    "| ---------------- | ------------------- | ------------------------------------------- |\n",
    "| **startproject** | 创建一个新工程      | `scrapy startproject <name> [dir]`          |\n",
    "| **genspider**    | 创建一个爬虫        | `scrapy genspider [options] <name><domain>` |\n",
    "| settings         | 获得爬虫配置信息    | `scrapy setting [options]`                  |\n",
    "| **crawl**        | 运行一个爬虫        | `scrapy crawl <spider>`                     |\n",
    "| list             | 列出工程中所有爬虫  | `scrapy list`                               |\n",
    "| shell            | 启动 URL 调试命令行 | `scrapy shell [url]`                        |\n",
    "\n",
    "为啥 Scrapy 采用命令行创建和运行爬虫？\n",
    "\n",
    "- 命令行（不是图形界面）更容易自动化，适合脚本控制\n",
    "- 本质上，Scrapy 是给程序员用的，功能（不是界面）更重要\n",
    "\n",
    "### 10.2 与 Request 库的比较\n",
    "**相同点**：\n",
    "- 两者都可以进行页面请求和爬取，python 爬虫的两个重要技术路线\n",
    "- 两者可用性都好，文档丰富，入门简单\n",
    "- 两者都没有处理 js、提交表单、应对验证码等功能（可扩展）\n",
    "\n",
    "| Requests                 | Scrapy                     |\n",
    "| ------------------------ | -------------------------- |\n",
    "| 页面级爬虫               | 网站级爬虫                 |\n",
    "| 功能库                   | 框架                       |\n",
    "| 并发性考虑不足，性能较差 | 并发性好，性能较高         |\n",
    "| 重点在于页面下载         | 重点在于爬虫结构           |\n",
    "| 定制灵活                 | 一般定制灵活，深度定制困难 |\n",
    "| 上手十分简单             | 入门稍难                   |\n",
    "\n",
    "**选用哪个技术路线开发爬虫？**\n",
    "- 非常小的需求，Requests 库\n",
    "- 不太小的需求（想形成自己的爬取库），Scrapy 框架\n",
    "- 定制程度很高的需求（不考虑规模），自搭框架，Requests > Scrapy\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46950f",
   "metadata": {},
   "source": [
    "### 10.3 Scrapy 爬虫基本使用\n",
    "#### 10.3.1 Scrapy 爬虫的使用步骤\n",
    "1. 创建一个工程和 Spider 模板\n",
    "2. 编写 Spider\n",
    "3. 编写 Item Pipeline\n",
    "4. 优化配置策略\n",
    "\n",
    "#### 10.3.2 Scrapy 爬虫的数据类型\n",
    "##### 10.3.2.1 Request 类\n",
    "`class scrapy.http.Request()`\n",
    "- Request 对象表示一个 HTTP 请求\n",
    "- 由 Spider 生成，由 Downloader 生成\n",
    "\n",
    "| 属性或方法 | 说明                                                 |\n",
    "| ---------- | ---------------------------------------------------- |\n",
    "| .url       | Request 对应的请求 URL 地址                          |\n",
    "| .method    | 对应的请求方法，’GET’’POST’等                        |\n",
    "| .headers   | 字典类型风格的请求头                                 |\n",
    "| .body      | 请求内容主体，字符串类型                             |\n",
    "| .meta      | 用户添加的扩展信息，在 Scrapy 内部模块间传递信息使用 |\n",
    "| .copy()    | 复制该请求                                           |\n",
    "\n",
    "##### 10.3.2.2 Response 类\n",
    "`class scrapy.http.Response()`\n",
    "- Response 对象表示一个 HTTP 响应\n",
    "- 由 Downloader 生成，由 Spider 处理\n",
    "\n",
    "| 属性或方法 | 说明                                  |\n",
    "| ---------- | ------------------------------------- |\n",
    "| .url       | Response 对应的 URL 地址              |\n",
    "| .status    | HTTP 状态码，默认是 200               |\n",
    "| .headers   | Response 对应的头部信息               |\n",
    "| .body      | Response 对应的内容信息，字符串类型   |\n",
    "| .flags     | 一组标记                              |\n",
    "| .request   | 产生 Response 类型对应的 Request 对象 |\n",
    "| .copy      | 复制该响应                            |\n",
    "\n",
    "##### 10.3.2.3 Item 类\n",
    "`class scrapy.item.Item()`\n",
    "\n",
    "- Item 对象表示一个从 HTML 页面中提取的信息内容\n",
    "- 由 Spider 生成，由 Item Pipeline 处理\n",
    "- Item 类似字典类型，可以按照字典类型操作\n",
    "\n",
    "#### 10.3.3 Scrapy 爬虫提取信息的方法\n",
    "- Beautiful Soup\n",
    "- lxml\n",
    "- re\n",
    "- XPath Selector\n",
    "- CSS Selector： `<HTML>.css('a::attr(href)').extract()`\n",
    "\n",
    "#### 10.3.4 yield 关键字\n",
    "- 生成器是一个不断产生值的函数\n",
    "- 包含 `yield` 语句的函数是一个生成器\n",
    "- 生成器每次产生一个值（`yield` 语句），函数被冻结，被唤醒后再产生一个值\n",
    "- 生成器相比列表（一次列出所有内容）的优势：\n",
    "  - 更节省存储空间\n",
    "  - 响应更迅速\n",
    "  - 使用更灵活"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7774ddfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 4 9 16 25 36 49 64 81 Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def gen(n):\n",
    "    \"\"\"\n",
    "    产生所有小于 n 的整数的平方值\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        yield i**2\n",
    "\n",
    "for i in gen(10):\n",
    "    print(i,'',end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7de5e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 4 9 16 25 36 49 64 81 Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def square(n):\n",
    "    ls = [i**2 for i in range(n)]\n",
    "    return ls\n",
    "\n",
    "for i in square(10):\n",
    "    print(i,'',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea746f20",
   "metadata": {},
   "source": [
    "### 10.4 Scrapy 爬虫实例\n",
    "#### 10.4.1 建立一个 Scrapy 爬虫工程\n",
    "1. 在 cmd 或者 pycharm 中输入 `scrapy startproject python123demo`\n",
    "2. 以 `python123demo` 定义一个工程目录名称，并会自动生成一系列文件\n",
    "\n",
    "**生成的工程目录**:\n",
    "\n",
    "<img src=\"https://gitee.com/lockegogo/markdown_photo/raw/master/202201241639437.png\" alt=\"image-20220124163942392\" style=\"zoom:80%;\" />\n",
    "\n",
    "#### 10.4.2 在工程中产生一个 Scrapy 爬虫\n",
    "在 `python123demo` 目录下输入命令 `scrapy genspider demo python123.io` 在 spider 目录下自动生成 `demo.py`（也可手动）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class DemoSpider(scrapy.Spider):\n",
    "    name = 'demo'\n",
    "    allowed_domains = ['python123.io']\t#用户提交给命令行的域名\n",
    "    start_urls = ['http://python123.io/']\t#所爬取的初始页面\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        处理响应，解析内容形成字典，发现新的 URL 爬取请求。\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fd9f6",
   "metadata": {},
   "source": [
    "#### 10.4.3 配置产生的 spider 爬虫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbe853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class DemoSpider(scrapy.Spider):\n",
    "    name = \"demo\"\n",
    "    #allowed_domains = ['python123.io']\n",
    "    start_urls = ['http://python123.io/ws/demo.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "        self 是面向对象类所属关系的标记\n",
    "        response 是从网页内容所存储或对应的对象\n",
    "        \"\"\"\n",
    "        fname = response.url.split('/')[-1]\n",
    "        with open(fname, 'wb')  as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Save file %s.' % name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857afbb0",
   "metadata": {},
   "source": [
    "#### 10.4.5 运行爬虫，获取网页\n",
    "在 `python123demo` 目录下输入命令 `scrapy crawl demo`，可以看到页面文件被存储到 `python123demo` 目录下"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc92c0",
   "metadata": {},
   "source": [
    "### 10.5 股票数据 Scrapy 爬虫实例\n",
    "与之前的 requests 库爬虫一样的实例，这次换 scrapy 框架实现。\n",
    "#### 10.5.1 建立工程和 Spider 模板\n",
    "- `>scrapy startproject scrapyStock`\n",
    "- `>cd scrapyStock`\n",
    "- `>scrapy genspider stocks baidu.com`\n",
    "- 进一步修改 `spiders/stocks.py` 文件\n",
    "\n",
    "#### 10.5.2 编写 Spider\n",
    "- 配置 stocks.py 文件\n",
    "- 修改对返回页面的处理\n",
    "- 修改对新增 URL 爬取请求的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5875cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import re\n",
    "\n",
    "class StocksSpider(scrapy.Spider):\n",
    "    name = 'stocks'\n",
    "    start_urls = ['http://quote.eastmoney.com/stock_list.html']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for href in response.css('a::attr(href)').extract():\n",
    "            try:\n",
    "                stock = re.findall(r\"[s][hz]\\d{6}\", href)[0]\n",
    "                # e.g. https://gu.qq.com/sh603102/gp\n",
    "                url = 'http://gu.qq.com/' + stock + '/gp'\n",
    "                # 重新提交 URL\n",
    "                yield scrapy.Request(url, callback=self.parse_stock)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    def parse_stock(self, response):\n",
    "        infoDict = {}\n",
    "        # 找到属性为 title_bg 的区域\n",
    "        stockName = response.css('.title_bg')\n",
    "        stockInfo = response.css('.col-2.fr')\n",
    "        # 然后在这一个区域中进一步检索 col-1-1 并且把相关的字符串提取出来\n",
    "        # e.g. N 百合\n",
    "        name = stockName.css('.col-1-1').extract()[0]\n",
    "        # e.g. 603102.SH\n",
    "        code = stockName.css('.col-1-2').extract()[0]\n",
    "        # 进一步搜索名称为 li 的标签\n",
    "        # 注意搜索属性前面有 ., 而搜索名称没有\n",
    "        info = stockInfo.css('li').extract()\n",
    "        for i in info[:13]:\n",
    "            key = re.findall('>.*?<', i)[1][1:-1]\n",
    "            key = key.replace('\\u2003', '')\n",
    "            key = key.replace('\\xa0', '')\n",
    "            try:\n",
    "                val = re.findall('>.*?<', i)[3][1:-1]\n",
    "            except:\n",
    "                val = '--'\n",
    "            infoDict[key] = val\n",
    "        # 注意 update 和直接赋值都能达到改变字典的目的\n",
    "        # 但是 update 的实现方式是遍历字典赋值，性能比直接赋值要低\n",
    "        # 只有在设计两个字典合并的操作，使用 dict1.update(dict2)\n",
    "        infoDict.update({' 股票名称': re.findall('\\>.*\\<', name)[0][1:-1] + \\\n",
    "                                 re.findall('\\>.*\\<', code)[0][1:-1]})\n",
    "        yield infoDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70eeaa1",
   "metadata": {},
   "source": [
    "#### 10.5.3 编写 Pipeline\n",
    "- 配置 pipelines.py 文件\n",
    "- 定义对爬取项（Scrapy Item）的处理类: 将爬取数据存储到文件中\n",
    "- 配置 ITEM_PIPELINES 选项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d329448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "class ScrapystockPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "\n",
    "class ScrapystockPipeline:\n",
    "    def open_spider(self, spider):\n",
    "        self.f = open('stock.txt', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.f.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        try:\n",
    "            line = str(dict(item)) + '\\n'\n",
    "            self.f.write(line)\n",
    "        except:\n",
    "            pass\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480ef4e",
   "metadata": {},
   "source": [
    "- settings 配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a971402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "    'scrapyStock.pipelines.ScrapystockPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0617c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最后执行程序\n",
    "scrapy crwal stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68310fdc",
   "metadata": {},
   "source": [
    "##### 10.5.4 Sarapy 爬虫实例优化\n",
    "配置并发连接选项（settings.py）：\n",
    "\n",
    "| 选项                           | 说明                                              |\n",
    "| ------------------------------ | ------------------------------------------------- |\n",
    "| CONCURRENT_REQUESTS            | Downloader 最大并发请求下载数量，默认 32          |\n",
    "| CONCURRENT_ITEMS               | Item Pipeline 最大并发 ITEM 处理数量，默认 100    |\n",
    "| CONCURRENT_REQUESTS_PER_DOMAIN | 每个目标域名最大的并发请求数量，默认 8            |\n",
    "| CONCURRENT_REQUESTS_PER_IP     | 每个目标 IP 最大的并发请求数量，默认 0，非 0 有效 |\n",
    "\n",
    "可修改参数以增加或降低爬取速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487fc18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7885372",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
